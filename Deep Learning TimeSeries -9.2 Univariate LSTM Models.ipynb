{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "## visualize model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30 40 50 60 70 80 90]\n",
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n"
     ]
    }
   ],
   "source": [
    "seq = np.arange(10, 100, 10)\n",
    "print(seq)\n",
    "#\n",
    "n_step = 3\n",
    "pred_step = 1\n",
    "def split_sequence(series, n_step, pred_step):\n",
    "    x_list, y_list = [], [] \n",
    "    for s in range(series.shape[0]-(n_step+pred_step-1)):\n",
    "        x_list.append(series[s:s+n_step]),\n",
    "        y_list.append(series[s+n_step])\n",
    "    X, y = np.array(x_list), np.array(y_list)\n",
    "    return X, y\n",
    "#\n",
    "X, y = split_sequence(seq, n_step, pred_step)\n",
    "for i in range(X.shape[0]):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.2 Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Key in the definition is the shape of the input; \n",
    "- that is what the model expects as input for each sample in terms of the number of time steps and the number of features.\n",
    "- We are working with a univariate series, so the number of features is one, for one variable.\n",
    "- The number of time steps as input is the number we chose when preparing our dataset as an argument to we chose when preparing our dataset as an argument to the split sequence() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/conda_env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "n_feature = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_step, n_feature)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The shape of the input for each sample is specified in the input shape argument on the definition of first hidden layer.\n",
    "- We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape: [samples, timesteps, features].\n",
    "- Our split sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we easily reshape it to have an additional dimension for the one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10]\n",
      " [20]\n",
      " [30]] 40\n",
      "[[20]\n",
      " [30]\n",
      " [40]] 50\n",
      "[[30]\n",
      " [40]\n",
      " [50]] 60\n",
      "[[40]\n",
      " [50]\n",
      " [60]] 70\n",
      "[[50]\n",
      " [60]\n",
      " [70]] 80\n",
      "[[60]\n",
      " [70]\n",
      " [80]] 90\n"
     ]
    }
   ],
   "source": [
    "X_lstm = X.reshape(X.shape[0], n_step, n_feature)\n",
    "for i in range(X_lstm.shape[0]):\n",
    "    print(X_lstm[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/conda_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f614a36f978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_lstm, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[70]\n",
      "  [80]\n",
      "  [90]]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "x_test = np.array([70, 80, 90])\n",
    "x_test_lstm = x_test.reshape(1, n_step, n_feature)\n",
    "print(x_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.419624]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(x_test_lstm)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='9_2_lstm.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vanilla LSTM model\n",
    "<img src=\"./9_2_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.3 Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n",
    "- An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n",
    "- We can address this by having the LSTM output a value for each time step in the input data by setting the return sequences=True argument on the layer.\n",
    "- This allows us to have 3D output from hidden LSTM layer as input to the next. We can therefore define a Stacked LSTM as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_stack = Sequential()\n",
    "model_stack.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_step, n_feature)))\n",
    "model_stack.add(LSTM(50, activation='relu'))\n",
    "model_stack.add(Dense(1))\n",
    "model_stack.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61482c8240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stack.fit(X_lstm, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.53959]]\n"
     ]
    }
   ],
   "source": [
    "y_hat_stack = model_stack.predict(x_test_lstm)\n",
    "print(y_hat_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_stack, to_file='9_2_3_lstm.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM model\n",
    "<img src=\"./9_2_3_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.4 Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./7-23-2019-bidir-lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.\n",
    "- This is called a Bidirectional LSTM. We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n",
    "- An example of defining a Bidirectional LSTM to read input both forward and backward is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_bidir = Sequential()\n",
    "model_bidir.add(Bidirectional(LSTM(50, activation='relu', input_shape=(n_step, n_feature))))\n",
    "model_bidir.add(Dense(1))\n",
    "model_bidir.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6130267eb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_bidir.fit(X_lstm, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.1807]]\n"
     ]
    }
   ],
   "source": [
    "yhat_bidir = model_bidir.predict(x_test_lstm)\n",
    "print(yhat_bidir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_bidir, to_file='9_2_4_lstm.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM model\n",
    "<img src=\"./9_2_4_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.5 CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180\n",
      " 190 200 210 220 230 240 250 260 270 280 290]\n"
     ]
    }
   ],
   "source": [
    "seq = np.arange(10, 300, 10)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30 40 50 60 70 80] 90\n",
      "[20 30 40 50 60 70 80 90] 100\n",
      "[ 30  40  50  60  70  80  90 100] 110\n",
      "[ 40  50  60  70  80  90 100 110] 120\n",
      "[ 50  60  70  80  90 100 110 120] 130\n",
      "[ 60  70  80  90 100 110 120 130] 140\n",
      "[ 70  80  90 100 110 120 130 140] 150\n",
      "[ 80  90 100 110 120 130 140 150] 160\n",
      "[ 90 100 110 120 130 140 150 160] 170\n",
      "[100 110 120 130 140 150 160 170] 180\n",
      "[110 120 130 140 150 160 170 180] 190\n",
      "[120 130 140 150 160 170 180 190] 200\n",
      "[130 140 150 160 170 180 190 200] 210\n",
      "[140 150 160 170 180 190 200 210] 220\n",
      "[150 160 170 180 190 200 210 220] 230\n",
      "[160 170 180 190 200 210 220 230] 240\n",
      "[170 180 190 200 210 220 230 240] 250\n",
      "[180 190 200 210 220 230 240 250] 260\n",
      "[190 200 210 220 230 240 250 260] 270\n",
      "[200 210 220 230 240 250 260 270] 280\n",
      "[210 220 230 240 250 260 270 280] 290\n"
     ]
    }
   ],
   "source": [
    "n_step = 8\n",
    "X4, y = split_sequence(seq, n_step, pred_step)\n",
    "for i in range(X4.shape[0]):\n",
    "    print(X4[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 10]\n",
      "   [ 20]\n",
      "   [ 30]\n",
      "   [ 40]]\n",
      "\n",
      "  [[ 50]\n",
      "   [ 60]\n",
      "   [ 70]\n",
      "   [ 80]]]\n",
      "\n",
      "\n",
      " [[[ 20]\n",
      "   [ 30]\n",
      "   [ 40]\n",
      "   [ 50]]\n",
      "\n",
      "  [[ 60]\n",
      "   [ 70]\n",
      "   [ 80]\n",
      "   [ 90]]]\n",
      "\n",
      "\n",
      " [[[ 30]\n",
      "   [ 40]\n",
      "   [ 50]\n",
      "   [ 60]]\n",
      "\n",
      "  [[ 70]\n",
      "   [ 80]\n",
      "   [ 90]\n",
      "   [100]]]\n",
      "\n",
      "\n",
      " [[[ 40]\n",
      "   [ 50]\n",
      "   [ 60]\n",
      "   [ 70]]\n",
      "\n",
      "  [[ 80]\n",
      "   [ 90]\n",
      "   [100]\n",
      "   [110]]]\n",
      "\n",
      "\n",
      " [[[ 50]\n",
      "   [ 60]\n",
      "   [ 70]\n",
      "   [ 80]]\n",
      "\n",
      "  [[ 90]\n",
      "   [100]\n",
      "   [110]\n",
      "   [120]]]\n",
      "\n",
      "\n",
      " [[[ 60]\n",
      "   [ 70]\n",
      "   [ 80]\n",
      "   [ 90]]\n",
      "\n",
      "  [[100]\n",
      "   [110]\n",
      "   [120]\n",
      "   [130]]]\n",
      "\n",
      "\n",
      " [[[ 70]\n",
      "   [ 80]\n",
      "   [ 90]\n",
      "   [100]]\n",
      "\n",
      "  [[110]\n",
      "   [120]\n",
      "   [130]\n",
      "   [140]]]\n",
      "\n",
      "\n",
      " [[[ 80]\n",
      "   [ 90]\n",
      "   [100]\n",
      "   [110]]\n",
      "\n",
      "  [[120]\n",
      "   [130]\n",
      "   [140]\n",
      "   [150]]]\n",
      "\n",
      "\n",
      " [[[ 90]\n",
      "   [100]\n",
      "   [110]\n",
      "   [120]]\n",
      "\n",
      "  [[130]\n",
      "   [140]\n",
      "   [150]\n",
      "   [160]]]\n",
      "\n",
      "\n",
      " [[[100]\n",
      "   [110]\n",
      "   [120]\n",
      "   [130]]\n",
      "\n",
      "  [[140]\n",
      "   [150]\n",
      "   [160]\n",
      "   [170]]]\n",
      "\n",
      "\n",
      " [[[110]\n",
      "   [120]\n",
      "   [130]\n",
      "   [140]]\n",
      "\n",
      "  [[150]\n",
      "   [160]\n",
      "   [170]\n",
      "   [180]]]\n",
      "\n",
      "\n",
      " [[[120]\n",
      "   [130]\n",
      "   [140]\n",
      "   [150]]\n",
      "\n",
      "  [[160]\n",
      "   [170]\n",
      "   [180]\n",
      "   [190]]]\n",
      "\n",
      "\n",
      " [[[130]\n",
      "   [140]\n",
      "   [150]\n",
      "   [160]]\n",
      "\n",
      "  [[170]\n",
      "   [180]\n",
      "   [190]\n",
      "   [200]]]\n",
      "\n",
      "\n",
      " [[[140]\n",
      "   [150]\n",
      "   [160]\n",
      "   [170]]\n",
      "\n",
      "  [[180]\n",
      "   [190]\n",
      "   [200]\n",
      "   [210]]]\n",
      "\n",
      "\n",
      " [[[150]\n",
      "   [160]\n",
      "   [170]\n",
      "   [180]]\n",
      "\n",
      "  [[190]\n",
      "   [200]\n",
      "   [210]\n",
      "   [220]]]\n",
      "\n",
      "\n",
      " [[[160]\n",
      "   [170]\n",
      "   [180]\n",
      "   [190]]\n",
      "\n",
      "  [[200]\n",
      "   [210]\n",
      "   [220]\n",
      "   [230]]]\n",
      "\n",
      "\n",
      " [[[170]\n",
      "   [180]\n",
      "   [190]\n",
      "   [200]]\n",
      "\n",
      "  [[210]\n",
      "   [220]\n",
      "   [230]\n",
      "   [240]]]\n",
      "\n",
      "\n",
      " [[[180]\n",
      "   [190]\n",
      "   [200]\n",
      "   [210]]\n",
      "\n",
      "  [[220]\n",
      "   [230]\n",
      "   [240]\n",
      "   [250]]]\n",
      "\n",
      "\n",
      " [[[190]\n",
      "   [200]\n",
      "   [210]\n",
      "   [220]]\n",
      "\n",
      "  [[230]\n",
      "   [240]\n",
      "   [250]\n",
      "   [260]]]\n",
      "\n",
      "\n",
      " [[[200]\n",
      "   [210]\n",
      "   [220]\n",
      "   [230]]\n",
      "\n",
      "  [[240]\n",
      "   [250]\n",
      "   [260]\n",
      "   [270]]]\n",
      "\n",
      "\n",
      " [[[210]\n",
      "   [220]\n",
      "   [230]\n",
      "   [240]]\n",
      "\n",
      "  [[250]\n",
      "   [260]\n",
      "   [270]\n",
      "   [280]]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_feature = 1\n",
    "n_seq = 2\n",
    "n_step = 4 # height\n",
    "X_cl = X4.reshape((X4.shape[0], n_seq, n_step, n_feature))\n",
    "print(X_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to reuse the same CNN model when reading in each sub-sequence of data separately.\n",
    "- This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence.\n",
    "- The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified.\n",
    "- The number of filters is the number of reads or interpretations of the input sequence.\n",
    "- The kernel size is the number of time steps included of each read operation of the input sequence.\n",
    "- The convolution layer is followed by a max pooling layer that distills the filter maps down to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "kernel_size = 2\n",
    "\n",
    "# define the input model\n",
    "model_cnnlstm = Sequential()\n",
    "model_cnnlstm.add(TimeDistributed(Conv1D(64, kernel_size, activation='relu'), input_shape=(None, n_step, n_feature)))\n",
    "model_cnnlstm.add(TimeDistributed(MaxPooling1D()))\n",
    "model_cnnlstm.add(TimeDistributed(Flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output model\n",
    "model_cnnlstm.add(LSTM(50, activation='relu'))\n",
    "model_cnnlstm.add(Dense(1))\n",
    "model_cnnlstm.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60bf7d3550>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnnlstm.fit(X_cl, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_23 (TimeDis (None, None, 3, 64)       192       \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, None, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 23,243\n",
      "Trainable params: 23,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnnlstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[220]\n",
      "   [230]\n",
      "   [240]\n",
      "   [250]]\n",
      "\n",
      "  [[260]\n",
      "   [270]\n",
      "   [280]\n",
      "   [290]]]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x_test_cl = np.array([220, 230, 240, 250, 260, 270, 280, 290])\n",
    "x_test_cl_reshape = x_test_cl.reshape(1, n_seq, n_step, n_feature)\n",
    "print(x_test_cl_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[316.13284]]\n"
     ]
    }
   ],
   "source": [
    "yhat_cl = model_cnnlstm.predict(x_test_cl_reshape)\n",
    "print(yhat_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_cnnlstm, to_file='9_2_5a_lstm.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM LSTM model\n",
    "<img src=\"./9_2_5a_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
